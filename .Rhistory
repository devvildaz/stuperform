summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 120,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 60,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 200,
batch_size= 32,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 32,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 120,
batch_size= 50,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 32,
activation="relu") %>%
layer_dense(name="DeepLayer4",
units= 16,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 120,
batch_size= 50,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 32,
activation="relu") %>%
layer_dense(name="DeepLayer4",
units= 16,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 120,
batch_size= 100,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 32,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 120,
batch_size= 150,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 128,
activation="relu") %>%
layer_dense(name="DeepLayer2",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 32,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 128,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer4",
units= 32,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 120,
batch_size= 150,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
library(readr)
library(keras)
library(DT)
library(fastDummies)
FILE_PATH <- "/home/devvildaz/Code/R/datasets/student_performance"
data.frame <- read.table(file.path(FILE_PATH, "student-mat.csv"), sep=";",header=TRUE)
sprintf("Rows (mat): %s", nrow(data.frame))
colnames(data.frame)[1:30]
datatable(data.frame)
sum(is.na(data.frame))
summary(data.frame)
factors_name <- c()
labels_cols <- c("G1", "G2", "G3")
features_cols <- c()
for(key in colnames(data.frame)){
if (class(data.frame[[key]]) == "factor"){
factors_name <- append(factors_name, key)
}
if (!key %in%  labels_cols) {
features_cols <- append(features_cols, key)
}
}
data.matrix <- as.matrix(data.frame)
set.seed(101) # semilla para la generacion random
indx <- sample(2, nrow(data.matrix), replace=TRUE, prob=c(0.70, 0.30))
library(caret)
x <- data.frame[features_cols]
y <- data.frame["G1"] + data.frame["G2"] + data.frame["G3"]
y <- y / 3
x <- dummy_cols(x, select_columns=factors_name, remove_selected_columns = TRUE, remove_first_dummy=TRUE)
x <- as.matrix(x)
params <- preProcess(x, method=c("center", "scale"))
x <-predict(params, x)
x_train <- x[indx == 1,]
x_test <- x[indx == 2,]
y_train <- y[indx == 1,]
y_test <- y[indx == 2,]
model <- keras_model_sequential()
model %>%
layer_dense(name="DeepLayer1",
units= ncol(x),
activation="relu",
input_shape=ncol(x) ) %>%
layer_dense(name="DeepLayer2",
units= 128,
activation="relu") %>%
layer_dense(name="DeepLayer3",
units= 64,
activation="relu") %>%
layer_dense(name="DeepLayer4",
units= 32,
activation="relu") %>%
layer_dense(name="OutputLayer",
units= 1,
activation="linear")
model %>% compile(loss="mse", optimizer="adamax", metrics=c("mean_squared_error"))
summary(model)
history <- model %>% fit(
x_train,
y_train,
epochs= 120,
batch_size= 150,
validation_split=0.25
)
model %>% evaluate (x_test, y_test)
pred <- model %>% predict(x_test)
cbind(round(pred), round(y_test))
temp <- sqrt(sum((pred - y_test)^2) / length(pred))
